{
  
    
        "post0": {
            "title": "FastCloud - the app",
            "content": "I am currently a student in the fast.ai practical deep learnign for coders v5 class and after the first week I built a cloud type classifier called FastClouds - which you can read about here . After the second and third lesson I have implemented some of the things Jeremy discussed and worked through and that is what I have to show you in this post. . Model Refinement . In lesson 2 of https://course.fast.ai/ v5 data cleaning and data augmentation was discussed. Data cleaning turned out to be a big part of making FastClouds perform better as the images i scraped from duckduckgo for clouds turned out to be quite messy and unreliably classified... unsurprising really. Once i&#39;d cleaned up my image data set my model went from ~75% accurate to ~81% accurate. . This is the training with raw data: . . This is the training with cleaned up data: . . Data augmentation using the fastai DataBlock was super simple to implement. I just added the batch_tfms=aug_transforms() line to my DataBlock definition and fastai took care of the rest for me. Data augmentation did add a bit of skill to the model - we went from ~81% to ~85% - but i suspect that this is because the input images are already quite varied and so augmentation didn&#39;t add that much more information for the model to learn. This is a batch of augmented images and they look very similar to the original images: . . Final performance . In the end we have a model that is about ~85% accurate using pretty much out of the box fastai methods and using a very small dataset - the final version only used 50 images from each category. As far as I know there is no other model out there that can classify clouds from images with this accuracy. . For reference, the notebook used is here: Kaggle Notebook and here GitHub Repo . TROPS . Now that we have a model i&#39;d like to be able to make it &#39;operational&#39; and available for people to use. The Transition to Operations (TROPS) using huggingface and gradio was discussed in lesson 2 and that is what i have used here to deploy the FastCloud model. . The app version looks like this: . . and you can go to this URL to play with it. . https://huggingface.co/spaces/robtheoceanographer/FastClouds . I&#39;d love it is you send me screenshots of the clouds you classify with this. Thanks for reading. .",
            "url": "https://robtheoceanographer.com/blog/fastai/jupyter/clouds/meteorology/2022/05/11/fastcloud-app.html",
            "relUrl": "/fastai/jupyter/clouds/meteorology/2022/05/11/fastcloud-app.html",
            "date": " • May 11, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "FastClouds - my first fastai model",
            "content": "I am currently a student in the fast.ai practical deep learnign for coders v5 class and after the first week I took Jeremy&#39;s advice to just try to complete some small project using the material covered so far in lesson 1. . Therefore, I humbly present FastClouds . This is not meant to be a serious project and is just for my own learning experience. . The Problem . On ground observations are a key part to weather forecasting. Most observations are taken by autonomous systems but there are still a few routine observations that are done manually by a human. One of these is cloud type classification. . This manual observation is currently done is at major airports around Australia. At these airports one or more highly knowledgeable and accredited aerodrome weather observers is stationed to take manual weather observations on a fixed schedule throughout each day. But, having such specialized observers at all airports all of the time is not cost effective or realistically feasible, especially for remote locations (e.g. uninhabited islands or infrequently used aerodromes). Therefore many of these remote or small areas miss out on observations and perhaps receive lower quality situational awareness and forecasts as a result. . The Solution . Using deep learning and image classification techniques to classifying cloud types from photographs seemed to me a very plausible solution to this problem. Therefore, after the Fastai course v5 lecture 1 I thought I&#39;d try to do exactly that using a visual learner example Jeremy provided as my starting point. . This algorithm uses a resnet and transfer learning as per the original notebook - [is-it-a-bird] -(https://www.kaggle.com/code/jhoward/is-it-a-bird-creating-a-model-from-your-own-data) but it uses three broad categories of clouds instead of just birds vs forests. These classes were chosen as per the work of Luke Howard in &quot;Essay of the Modifications of Clouds&quot; (1803). . This is an image from: https://www.weather.gov/jetstream/corefour . In order to create a data set duckduckgo was searched for the terms: cirrus clouds, cumulus clouds, stratus clouds. here is an example batch of images used in the training: . . The Notebook . This notebook is sitting on Kaggle as it has a nice (e.g. free...) gpu backend that allows you to run this model should you wish to poke around. . So, here it is for you to enjoy - https://www.kaggle.com/robtheoceanographer/fastclouds . Conclusion . While this is very basic demo and only about 70% accurate, it shows great potential for automating some of the cloud type observing in remote regions of Australia where there is currnetly limited or no human observer coverage, and therefore this work might benifit from future work. . I&#39;d love ideas, feedback, and suggestions should anyone have any. . Thanks .",
            "url": "https://robtheoceanographer.com/blog/fastai/jupyter/clouds/meteorology/2022/04/29/fastcloud.html",
            "relUrl": "/fastai/jupyter/clouds/meteorology/2022/04/29/fastcloud.html",
            "date": " • Apr 29, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "A template for structuring your scientific code",
            "content": "Over the last ten years or so i’ve slowly settled on a standard template and directory structure for my coding projects. I don’t see many people sharing ideas on project layout or directory structure so I thought i’d post a short blog on what I do. . Small to Medium Projects . In my mind small project are projects where all of the data and source code can fit on my laptop without causing too many problems. Therefore i put the whole project in one project directory structured as so: . . |-- bin |-- config.ini |-- docs |-- data |-- logs |-- runtime | `-- logs |-- scripts | `-- dotask.py | `-- python | `-- runall.py | `-- shell |-- tests . Small - all in one dir on the one machine. Mid - most in one dir but data maybe on an external drive. . Big Projects . Huge - some code with data on one machine some other code and data on another machine. Try to have src all together and to have the whole project managed by version control and make sure that the you know what is checked out where. Maybe have a build script that you check out in a tmp dir and run build to move it to wherever you need it to be. Have no hard code paths - only use config files that allow you to modify stuff on the fly as the time will come when you want to run an experiment and put data elsewhere. .",
            "url": "https://robtheoceanographer.com/blog/markdown/2021/09/05/ProjectDirTemplate.html",
            "relUrl": "/markdown/2021/09/05/ProjectDirTemplate.html",
            "date": " • Sep 5, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Grow your own Python Package",
            "content": "I decided to write up this short post to help both me and some colleagues remember what the most basic way of constructing a python package is. . This is a bare-bones build - you should be doing more than this - for example putting in README file and LICENSE file and good docstrings. . The basic structure of a python package is quite straight forward: . basemypackage --&gt; Base └── mypackage --&gt; Actual Module ├── extras │ ├── multiply.py │ ├── divide.py ├── add.py ├── subtract.py . Here we have a dir called mypackage which has two python files in it - add.py and subtract.py but it also has a sub dir called extras which contains two more python files. all of this will form up the basis of our imaginary python package. keep in mind that you will have different files and folders in your package and you can divide up the files/folders in as course or as fine a way as you like. . The init.py . You will always find one or several __init__.py files in python packages. this is because the __init__.py files tell python to treat directories as modules (or sub-modules as we will see in a minute). this makes it an important part of our package and one we need to spend some time looking at. . The __init__.py file contains two important things: . a doc string for the available subpackages/submodules/methods in this dir (see the one in numpy as an example: https://github.com/numpy/numpy/blob/main/numpy/__init__.py) | the names of all the methods in all the Python files that are in this immediate directory. | . A typical __init__.py looks like this: . &quot;&quot;&quot; some helpful info of what each method is/does... &quot;&quot;&quot; from file import method # &#39;method&#39; is a function found in the python file called &#39;file.py&#39; . You will need to put a __init__.py in each sub dir of your package and so each sub-dir will become sub-modules of your main package. . here is the specific __init__.py for our dummy example: . &quot;&quot;&quot; Provides 1. ability to add two numbers 2. ability to subtract two number. ... &quot;&quot;&quot; from add import add from subtract import subtract . as we have an extras sub dir we need one in there too: . &quot;&quot;&quot; Provides 1. ability to add multiply numbers 2. ability to divide number. ... &quot;&quot;&quot; from multiply import multiply from divide import divide . The setup.py . Another attribute of a python package is the setup.py which is a python file that contains information about your package, it’s version, its dependencies, and a whole whole lot more. . Within the basemypackage dir (and in the same directory as our module mypackage ) we will now add a setup.py file: . from setuptools import setup, find_packages VERSION = &#39;0.0.1&#39; DESCRIPTION = &#39;My absolutely gorgeous Python package&#39; LONG_DESCRIPTION = &#39;My Python package, it is gorgeous, it makes you just want to import it.&#39; # Setting up setup( # the name must match the folder name &#39;mypackage&#39; name=&quot;mypackage&quot;, version=VERSION, author=&quot;Marty McFly&quot;, author_email=&quot;Marty.McFly@deloreantimemachines.com&gt;&quot;, description=DESCRIPTION, long_description=LONG_DESCRIPTION, packages=find_packages(), install_requires=[], # add any additional packages that # needs to be installed along with this package. Eg: &#39;numpy&#39; keywords=[&#39;python&#39;, &#39;gorgeous&#39;], url=&#39;https://gitlab.com/mygroup/gorgeous/basemypackage&#39;, license=&#39;MIT&#39;, classifiers= [ &quot;Development Status :: 1 - Planning&quot;, &quot;Intended Audience :: Religion&quot;, &quot;Programming Language :: Python :: 3&quot;, &quot;Operating System :: Unix&quot;, ] ) . The setuptools.setup() method accepts a variety of keyword arguments to specify additional metadata about your package. - info on this can be found here: https://github.com/pypa/sampleproject/blob/main/setup.py. This includs a classifiers section - a full list of classifiers can be found here: https://pypi.org/pypi?%3Aaction=list_classifiers. . Build/Install it . You can now build and install your package locally using the dir you have just set up. It is a better idea to store this package on-line somewhere where it can be version controlled and available whenever you need it. Many people build and distribute their package via PyPi - as it is the official Python repository where all Python packages are stored - but i find that a little scary and tend to just do my own private thing using gitlab so i’m not going to show you that. First we will cover just local build/install and then get to that other stuff later. . Locally . It’s common to locally install your project in “editable” or “developer” mode while you’re still developing/working on it. This allows your project to be both installed and editable so that updates are available when they get made. . Within the basemypackage dir run: . python -m pip install -e . . Although somewhat cryptic, -e is short for –editable, and . refers to the current working directory, so it should install the current directory in editable mode. This will also install any dependencies declared in the setup.py. . You should now be able to import mypackage in your python environment. . GitLab . It is more useful not to have a local copy of the package source-code but to use a version control system like gitlab to manage a lot of that for you. This is my preferred method of working and allows you to control who installs your package. . If you don’t already have one, create a new gitlab project and clone it to your local machine. Put all of the code that you want to include in your package including all the stuff shown above into this folder. . pip can talk git directly so it’s quite easy to build our python package from gitlab. Here is an example: . python3 -m pip install -e git+ssh://git@gitlab.com/mygroup/gorgeous/basemypackage.git . Or if you want to specify a non default branch: . python3 -m pip install git+ssh://git@gitlab.com/mygroup/gorgeous/basemypackage.git@dev . Or if you want to fetch a particular tag: . python3 -m pip install git+ssh://git@gitlab.com/mygroup/gorgeous/basemypackage.git@v0.01 . .N.B. The : at the end of gitlab.com must be changed to a / and you must have an ssh key registered with gitlab.com for the machine you’re working on. . There are a lot of other ways to install from version control that you can read about here: https://pip.pypa.io/en/stable/reference/pip_install/#vcs-support. . Notes . What i haven’t covered here is testing and adding test running to your package. This will come in the future i expect. .",
            "url": "https://robtheoceanographer.com/blog/markdown/2021/04/19/PythonPackaging.html",
            "relUrl": "/markdown/2021/04/19/PythonPackaging.html",
            "date": " • Apr 19, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Collaborative Coding with Git & GitLab",
            "content": "I’ve been asked by more than one person about how to use git and merge requests for collaborative scientific coding. Here are my notes for me to refer back to later and for me to point people to when I spend time teaching them how to do this. . What and Why? . Version control is the lab notebook of the digital world: it’s what professionals use to keep track of what they’ve done and to collaborate. Every large software development project relies on version control, and most scientists and engineers use it for their work as well. But, it isn’t just for software: books, papers, small data sets, and anything that changes over time or needs to be shared can and should be stored in some sort of version control system. . A version control system keeps track of these changes for us by creating different versions of our files. It allows us to decide which changes will be made to the next version (each record of these changes is called a commit), and keeps useful metadata about them. The complete history of commits for a particular project and their metadata make up a repository. Repositories can be kept in sync across different computers which facilitates collaboration among different people. . Version control is an unlimited ‘undo’ button, but it also allows many people to work in parallel. . How? . Ok, so how do we actually do this? At the moment most people use git and so that is what we will focus on here, but keep in mind that there are other version control systems out there. . Creating an repository . Setup . When we use git on a computer for the first time, we need to configure a few things: . $ git config --global user.name &quot;your name&quot; $ git config --global user.email &quot;you@thereis.noplace.likehome&quot; . Please use your own name and email address instead of my dummy ones. From here on in this user name and email will be associated with your login and will be used for all your subsequent git activity. . It is also a good idea to set up which editor git should use on this machine too: . git config --global core.editor &quot;vim&quot; . You can check your settings at any time: . git config --list . Create something . First, let’s create a directory in our home dir to do some work in: . cd ~/ mkdir the_next_spaceX cd the_next_spaceX . Then we tell git to make this dir a git repository – that is a place where git can/should track and store versions of our files: . git init . Note - this is only needed at the root dir of a project as git tracks all subfolders of this dir. . Now lets look at what that has done: . ls . Hmm… nothing…???? . well, lets look a little closer: . ls -a . It turns out that git has created a hidden directory within our dir called .git. . Git uses this hidden subdirectory to store all the information about the repo, including all files and sub-directories located within the project’s directory. If you ever delete the .git subdirectory you will lose the all of the repos’s history. It is an important dir. . Check that everything is set up and working: . git status . This has to be one of my most typed git commands as it tells you what git is doing and thinking. We will talk about what all this output means soon. . Tracking your work . Ok so you should be in our the_next_spaceX directory and ready to do some cool work. Let’s create a file called mars.txt that contains some notes about the planets suitability as a base. We’ll use vim to edit the file (you can use whatever editor you like). . vim mars.txt . Now check it has been created properly: . ls . yep it is there. So, what does git think about this file? . git status . Git tells us that it’s noticed the new file but that it is untracked. untracked files just means that there’s a file in the directory that Git isn’t keeping track of and we need to decided if we want git to care about it or not. Tell Git to track a file using: . git add mars.txt . and the check that it worked: . git status . So, now git now knows that we want it to keep track of mars.txt, but it is telling us that it hasn’t recorded the changes we made to it as a commit yet. To get it to actually record the changes, we need to run one more command: . git commit -m &quot;Started my notes on Mars as a base&quot; . When we run git commit, git takes everything we have told it to save by using git add and stores a copy permanently inside that special .git directory. This permanent copy is called a commit and its short identifier is some unique number/letter combo like f22b25e. This identifier allows us to find this point in history later. . Just a few explanatory notes - the -m flag (for “message”) is used to record a short descriptive comment that will help us remember what we did and why. If we just run git commit without the -m option then git will launch our pre-set editor and ask us to type message. Good commit messages start with a brief (&lt;50 characters) statement about the changes made in the commit. I like to use semantic commit messages but you can use whatever you like - some people prefer sentences like “If applied, this commit will…”. Always write more than you think is needed. . What has this commit done: . git status . It has put all our changes into git and the message tells us that git is up to date with the files in this folder. . If we want to know what we’ve done recently, we can ask Git to show us the repo’s history: . git log . This lists all commits made to a repository in reverse chronological order and shows you the long versions of those commit identifiers mentioned above. . History and how to undo-it . As we mentioned above - we can refer to commits by their identifiers. You can refer to the most recent commit of the working directory by using the identifier HEAD. . (N.B. make some random changes to the file here so that it is different and these examples work…) . Lets look at our work progress using the HEAD reference: . git diff HEAD mars.txt . This is the same as if you leave out HEAD (try it). The point of showing this it to make it clear that we can refer to previous commits. An easy way to look at the difference between now and previous commits is by adding ~1 (where “~” is “tilde”, pronounced [til-duh]) to refer to the commit one before HEAD. . git diff HEAD~1 mars.txt . If we want to see the diff between even older commits we can use git diff again, but with the notation HEAD~1, HEAD~2, HEAD~3, and so on: . git diff HEAD~3 mars.txt . If we aren’t interested in the specific difference between commits we can use git show which shows us what changes we made at an older commit as well as the commit message that we wrote at the time: . git show HEAD~3 mars.txt . But what if we don’t know how many commits ago the change we’re interested in was? Well we can also refer to commits using those long strings of digits and letters called identifiers that git log displays: . git diff f22b25e3233b4645dabd0d81e651fe074bd8e73b mars.txt . So we can see what we’ve changed but how can we restore older versions of things? Let’s suppose we change our mind about the last update to mars.txt and want to roll-back to a previously committed version. . Lets look at the status of our git repo to see where we’re at: . git status . We have un-staged changes and we will now put things back the way they were: . git checkout HEAD mars.txt . git checkout checks out (i.e., restores) an old version of a file. In this case, we’re telling git that we want to recover the version of the file recorded in the HEAD commit, which is the last saved commit. If we want to go back even further, we can use a commit identifier just like we did with diff and show above: . git checkout f22b25e mars.txt . Lets see what our git system is thinking about all this jumping around: . git status . This shows us that the file has been changed and are now sitting in the staging area ready for us to do something with. . Word of warning… . There is a risk of getting in to a ‘detached HEAD’ state when moving back in time with git. We can to revert our file to its state in a previous commit, but be careful! The command checkout has other important functionalities within git and it is easy for git to misunderstand your intentions if you are not very accurate with what you ask for. For example, if you forget to add the filename - eg. mars.txt in the previous command, you will get an error saying something about: . You are in &#39;detached HEAD&#39; state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout... . Don’t panic - the “detached HEAD” state is like “look, but don’t touch” space where so you shouldn’t make any changes in this state and after investigating your repo’s past state you should reattach your HEAD with git checkout master. . Completely reverting a committed change . The above checkout commands allow you to modify one file in the stage area but what if you want to completely revert a commit? Lets say that the last commit to the project’s repository contained an error and you wants to undo it. git revert [erroneous commit ID] will create a new commit that reverses the erroneous commit. Therefore git revert is different to git checkout [commit ID] because git checkout returns the files within the local repository to a previous state, whereas git revert reverses changes committed to the local and project repositories. . Ignore stuff . We almost always have files that we do not want git to track for us - like backup files created by our editor or large data files . (N.B. Let’s create a few dummy files so we have something to ignore…) . mkdir results touch a.dat b.dat c.dat results/a.out results/b.out . Lets see what git sees now: . git status . Now let’s pretend that putting these files in git would be a waste of disk space, so let’s tell git to ignore them. We do this by creating a file in the root directory of our project called .gitignore: . vim .gitignore . add the following to this file: . *.dat results/ . These wildcard patterns tell git what to ignore. Now that we have created this file we need to commit it to our repo so that it stays with our repo. . I almost never craft a .gitignore manually and I almost always steal them from other people - see lots of examples here: https://github.com/github/gitignore . Remotes . Version control really comes into its own when we begin to collaborate with others. We already have most of the machinery we need to do this. The only thing missing is to copy changes from your local repository to another repository on another machine/place. . The beauty of git is that it allow us to easily move work between any number of repositories in any location. In practice, though, it is much saner/easier to use one copy as a “central hub”, and to keep it on the web rather than on someone’s private machine/laptop. Most people use online hosting services like GitHub, Bitbucket, or GitLab, to hold those central or master copies of the repo. . Let’s start by setting up a GitLab repo for sharing the changes we’ve made to our current project with the rest of the team. Log in to GitLab.com, then click on the “New Project” icon in the top right corner then create a blank project called the_next_spaceX. . . As soon as the repository is created GitLab displays a page with a URL and some information on how to configure your local repository. . What you have effectively done is the following on GitLab’s servers: . mkdir the_next_spaceX cd the_next_spaceX git init . and have created an empty git repo with your projects name. Now we need to connect our local git repo with the new, but empty, GitLab one. We do this by making the GitLab repository a remote for the local repository. The home page of the repository on GitLab includes the command you need to run on your machine: . git remote add origin git@gitlab.com:ccfp1/the_next_spacex.git . Note - origin is a just a name used to refer to the remote repository. It could be called anything, but origin is a convention that is often used by default in git and GitLab, so it’s helpful to stick with this unless there’s a reason not to. . We can check that the command has worked: . git remote -v . Once the remote is set up, this command will push the changes from our local repository to the repository on GitLab: . git push origin master . We can pull changes from the remote repository to the local one as well: . git pull origin master . We now have a way to share (some people call it ‘inflict’) your code with others. . Collaborating . For this part it is good to work in pairs… One person will be the “Owner” and the other will be the “Collaborator”. The goal is that the Collaborator add changes into the Owner’s repository. . (Note - the Owner needs to give the Collaborator access to the repo) . Now the Collaborator needs to download a copy of the Owner’s repository to their machine. This is called “cloning a repo”. To clone the Owner’s repo: . git clone https://gitlab.com/ccfp1/the_next_spacex.git ~/the_next_spacex . The Collaborator can now make a change in their clone of the Owner’s repository, exactly the same way as we’ve been doing before: . cd ~/the_next_spacex vim pluto.txt . Now add this to your local copy of the repo: . git add pluto.txt git commit -m &quot;Add notes about Pluto&quot; . Now we share this back with the remote repo by “pushing” the change to the Owner’s repository on GitLab: . git push origin master . Ok, so now look to the Owner’s repository on GitLab website now. You should be able to see the new commit made by the Collaborator. . Now you as the owner can download the Collaborator’s changes from GitLab: . git pull origin master . Now the three repositories (Owner’s local, Collaborator’s local, and Owner’s on GitLab) are in sync. . A Basic Collaborative Workflow: . In practice, it is good to be sure that you have an updated version of the repository you are collaborating on, so you should git pull before making our changes. The basic collaborative workflow would be: . update your local repo with git pull origin master, | make your changes and stage them with git add, | commit your changes with git commit -m, and | upload the changes to GitLab with git push origin master | . It is better to make many commits with smaller changes rather than of one commit with massive changes: small commits are easier to read and review. . Branching . How do we all work on the same code/repo/project at the same time without clobbering each other… enter branching… . I think GitHub Flow is a good and simple way to do this, but there’s a lot of different workflows for git branching so search around and you might find one you prefer. . You always start from the master branch - which is the most stable and working version of you code - checking out a new branch to do your next piece of work. When you’re ready for your work to be reviewed and merged into the master branch of the project you open a merge request to master. . N.B. Even if you’re a solo developer on a project, I still recommend you use this the process of making branches and opening merge requests as it serves as a record of your work - it is your lab-notebook. . When you create a branch in your project, you’re creating an isolated copy/environment where you can try out your new ideas. Changes you make on one branch do not affect the other branches, so you are free to experiment and commit changes, safe in the knowledge that your branch won’t be merged until it’s ready to be reviewed by someone you’re collaborating with. . N.B. Branching is a core concept in git and the entire git workflow is based upon it. There’s only one rule: anything in the master branch is always deployable/usable. . Creating branches . Lets actually create a branch: . We will do this in the gitlab webpage... make an &quot;update-mars-file&quot; branch (#TODO insert a screenshot here) . Your branch name should be descriptive (e.g., parallelize-function-a), so that others can see what is being worked on. . Now, once your branch has been created, it’s time to start making changes… pull your branch to your machine: . What branch are we on now? . git status . Now fetch the upstream changes: . git fetch . git fetch downloads commits, files, and refs from the remote repository into your local repo. Fetching is what you do when you want to see what everybody else has been working on. Git isolates fetched content from existing local content and it has absolutely no effect on your local development work. Fetched content has to be explicitly checked out using the git checkout… lets do that now: . git checkout update-mars-file . You can now make your changes in this ‘copy’ of the code and commit, push/pull until your heart is content. . You can also work on many branches at once but “checking-out” the other branches when you want to work on them. . Merging &amp; Review . “Merge requests” initiate a discussion about the commits you made in your branch. . You can open a Merge Request at any point: when you have little or no code but you just want to share some general ideas, or when you’re stuck and need help or advice, or when you’re ready for someone to review your work. By using GitLab’s @mention system in your Merge Requests and messages, you can ask for feedback from specific people or teams. . Lets do it - first push your local code back up to the remote: . git push origin update-mars-file . Now go to the gitlab webpage and create a merge request: . we will do this on the gitlab webpage... (#TODO insert a screenshot here) . Once a Merge Request has been opened, the person or team reviewing your changes will have questions or comments. Merge Requests are designed to encourage and capture these conversation and feedback. . N.B. You can also continue to push to changes this branch in light of discussion and feedback about your commits. If someone comments that you forgot to do something or if there is a bug in the code, you can fix it in your branch locally and push up the change. GitLab will show your new commits and any additional feedback you may receive in the unified Merge Request view. . do a simple review process on gitlab webpage... and then merge it... (#TODO insert a screenshot here) . Once your Merge Request has been reviewed and the branch passes your tests, you can merge your changes into master which will eventually be used as a release to “production”. Deploying to production won’t be covered here. Once merged, “Merge Requests” preserve the record of the historical changes to our code. Because they’re search-able, discoverable, and they let anyone go back in time to understand why and how a decision was made. . Acknowledgments . Most of this content came from the wonderful work of Software-Carpentry https://swcarpentry.github.io/git-novice/ that i have mixed and matched and extended into bracnhing and review. .",
            "url": "https://robtheoceanographer.com/blog/markdown/2021/03/21/CollaborativeCoding.html",
            "relUrl": "/markdown/2021/03/21/CollaborativeCoding.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://robtheoceanographer.com/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Python docstrings - An Absolute Beginners Guide",
            "content": "I’ve been asked a few times recently about how to write good docstrings in Python so I thought I’d put a few notes up for everyone to refer back to later. . What is it? . There’s no need to re-invent the wheel here so the best thing is to read the python definition on what-is-a-docstring. . But, having said that I will highlight the purpose of a docstring for those of you, like me, who are too lazy to bother reading those pages - a docstring is a piece of text that occurs as the first line(s) in a python function, class, or module. This text becomes the __doc__ special attribute of that object and is used to help users know what the thing you wrote is, does, and needs. Without a docstring your work is pretty useless and almost completely un-reusable and un-sharable. For me, docstrings are all about the re-usability and sharability. . Functions . Every function you write needs a docstring - in fact I’d say this is the most important thing you can do when writing a function (aside from testing, but you’re already doing that…right?). . A good docstring contains three elements: . a one line description of the purpose. | a clear description of the inputs and outputs to the function. | the type each input and output will be. | Here is how to write a good docstring for a function: . def add_binary(a, b): &#39;&#39;&#39; Returns the sum of two decimal numbers in binary digits. Args: a (int): A decimal integer b (int): Another decimal integer Returns: binary_sum (str): Binary string of the sum of a and b &#39;&#39;&#39; binary_sum = bin(a+b)[2:] return binary_sum . The docstring has now become the objects __doc__ attribute and can now be used by users to get help on the use of the function: . print(add_binary.__doc__) . output: . Returns the sum of two decimal numbers in binary digits. Args: a (int): A decimal integer b (int): Another decimal integer Returns: binary_sum (str): Binary string of the sum of a and b . Classes . Most people are good at putting docstrings in their function but people often forget to put them in the classes they write. The docstring for a class is very similar to the function docstring but it has two extra elements: . a description of the basic behavior of the class. | a list of the public methods available to the user. | Here is an example of a class docstring: . class Person: &quot;&quot;&quot; A class to represent a person. some blah about why/what it does. Args: name (str): The first name of the person surname (str): The family name of the person age (int): The age of the person Methods: info(additional=&quot;&quot;): Prints the person&#39;s name and age. &quot;&quot;&quot; def __init__(self, name, surname, age): &quot;&quot;&quot; Constructs all the necessary attributes for the person object. Args: name (str): The first name of the person surname (str): The family name of the person age (int): The age of the person &quot;&quot;&quot; self.name = name self.surname = surname self.age = age ... . As per the function docstring this text is also assigned to the __docstring__ attribute for the class. we can see that by: . print(Person.__doc__) . Output . A class to represent a person. some blah about why/what it does. Args: name (str): The first name of the person surname (str): The family name of the person age (int): The age of the person Methods: info(additional=&quot;&quot;): Prints the person&#39;s name and age. . Modules . If people are slack with class docstring then they are completely hopeless when it comes to docstrings in Modules. Module docstrings are much like a Class docstring except that it also includes descriptions of the classes available in the module. This is best way to understand this is with an example. Here we will look at the pickle module: . import pickle print(pickle.__doc__) . output . Create portable serialized representations of Python objects. See module cPickle for a (much) faster implementation. See module copy_reg for a mechanism for registering custom picklers. See module pickletools source for extensive comments. Classes: Pickler Unpickler Functions: dump(object, file) dumps(object) -&gt; string load(file) -&gt; object loads(string) -&gt; object Misc variables: __version__ format_version compatible_formats . Here, we can see the docstring that is written at the beginning of the pickle.py module file which has been assigned to the __doc__ attribute for the module and can be be accessed as its docstring. . Help() . There is a help() function in python that leverages all of these docstrings and aggregates them into a nice little manual for the user to explore your code. Again, Pickle is a good example: . import pickle help(pickle) . I won’t paste the output here as it’s too long… go run it yourself and see what happens… . More Info . If you need inspiration check out pep 257 – Docstring Conventions or go hunting around in your favourite python module. .",
            "url": "https://robtheoceanographer.com/blog/markdown/2020/01/11/HowToDocstring.html",
            "relUrl": "/markdown/2020/01/11/HowToDocstring.html",
            "date": " • Jan 11, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Checklist for Technological Innovation in Data Science",
            "content": "Lately I’ve been thinking about how one decides to adopt new technology or change the way one works. There’s a lot of push for Data Scientists to keep up with the latest and greatest innovation in IT or analysis (e.g. change to git rather than staying with svn) but there’s rarely any objective analysis or thought put into whether or not a Data Scientist should invest the time and energy in doing this. Therefore I decided to put together a list of principles that I hold and use a a yard stick when presented with such situations. These are presented below. . The new tool or method should be cheaper than the one it replaces. . | It should be at least as fast as the tool or method it replaces. . | It should do work or produce work that is clearly and demonstrably ‘better’ than the one it replaces. . | It should be repairable and maintainable by people inside the organisation, provided that they are provided with the necessary tools. . | It should not replace or disrupt anything good that already exists, and this includes people and community relationships. . |",
            "url": "https://robtheoceanographer.com/blog/markdown/2018/03/27/InnovationChecklist.html",
            "relUrl": "/markdown/2018/03/27/InnovationChecklist.html",
            "date": " • Mar 27, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "A Test Driven Dev Example",
            "content": "How do you code? . Research (see here) shows that: . 1) the knowledge required to develop and use scientific software is primarily acquired from peers and through self-study, rather than from formal education and training; | 2) the number of scientists using supercomputers is small compared to the number using desktop or intermediate computers; | 3) most scientists rely primarily on software with a large user base; | 4) while many scientists believe that software testing is important, a smaller number believe they have sufficient understanding about testing concepts; | . This blog is a small example of test-driven development and will hopefully help address point 4. . Take away: . If you only take one thing from this post it’s that everytime you do any coding ask yourself two questions: . How do I (and you) trust this piece of code? | How do I (and you) know it is doing what intended it to do? | . Why Test? . “We believe that software is just another kind of experimental apparatus and should be built, checked, and used as carefully as any physical apparatus. However, while most scientists are careful to validate their laboratory and field equipment, most do not know how reliable their software is.” source . You wouldn’t use an instrument that hasn’t been calibrated but we seem to have no problem using code without any check or balances at all. . Test Driven Development . Test-driven Development (TDD) is a procedure that takes the process of writing code and the writing tests for it and flips it on it’s head. It advocates that you write the tests first and the least amount of code possible to pass these tests second. . Before you write a single line of a function, you first write the test for that function. . After you’ve written the test you can then write the smallest amount of code needed to pass that test. If you want to do more or add more functionality you must first write another test and so on repeating the test-then-implement process until you have working code - often with as much testing code as actual code. . Most of the online info about TDD shows examples using testing harnesses and automated testing libraries. when i was starting out i found these very hard to follow and to use. Most scientist develop interactively in through an ide or terminal and so here i want to provide an example that can implement the principles of TDD in this framework. . The example: . import pandas import numpy . Non-test driven development . # load my data. some_data=pandas.read_csv(&#39;simple_data.csv&#39;, sep=&#39;,&#39;,header=None) . # divide column two by 2.0. some_data[1]/2.0 . TypeError Traceback (most recent call last) &lt;ipython-input-3-15ba6180a4ae&gt; in &lt;module&gt;() 1 # divide column two by 2.0. -&gt; 2 some_data[1]/2.0 ... TypeError: unsupported operand type(s) for /: &#39;str&#39; and &#39;float&#39; . ####_ We are already starting to run into errors that are making me go back and troubleshoot/debug._ . Defensive dev version . My Plan . 1) Load data . Use my knowledge of the dataset to write a test upfront that will all me to be certain that the data has loaded properly. . 2) Divide the second column of data by 2. . This will require a function that can do the math and then the application of that function to my data . My Pre-defined Tests . # 1) loading of data. # I know that the data should have two columns and 39 rows when loaded so lets test that. some_data.shape == (39,2) . $ False . # 2) I know that when I divide the number 5 I should get 2.5 so lets test the divide function using that. divide_by_two(5) == 2.5 . NameError Traceback (most recent call last) &lt;ipython-input-5-c169179b0f66&gt; in &lt;module&gt;() 1 # 2) I know that when I divide the number 5 I should get 2.5 so lets test the divide function using that. -&gt; 2 divide_by_two(5) == 2.5 NameError: name &#39;divide_by_two&#39; is not defined . Redo the tests to be more useful and verbose . Nicer and more readble way to write tests like these is to use some inbuilt tools. . One of the easiest and most acessible is the assert statement. It is designed to catch conditions that should never occur in your code and to allow your code to stop at that point and return you a sensisble message. . Python has assert(), http://swcarpentry.github.io/python-novice-inflammation/08-defensive/ | R has assert() but it’s in a package called testit (https://cran.rstudio.com/web/packages/testit/index.html), | Matlab has assert() but i don’t know how to use it… | . In python the basics of assert are: . assert x &gt;= 0, &#39;x is less than zero&#39; . when x is less than zero the logic of x &gt;= 0 will return false and an assert error will be raised printing the message that appears after the comma. . # 1) loading of data. assert some_data.shape == (39,2), &quot;The data hasn&#39;t loaded properly. Size is not 39 x 2&quot; . AssertionError Traceback (most recent call last) &lt;ipython-input-6-b2d7fe176e23&gt; in &lt;module&gt;() 1 # 1) loading of data. -&gt; 2 assert some_data.shape == (39,2), &quot;The data hasn&#39;t loaded properly. Size is not 39 x 2&quot; AssertionError: The data hasn&#39;t loaded properly. Size is not 39 x 2 . # 2) I know that when i divide the number 5 i should get 2.5 so lets test the divide function using that. assert divide_by_two(5) == 2.5, &quot;The divide_by_two function did not return 2.5 when given 5.&quot; . NameError Traceback (most recent call last) &lt;ipython-input-7-533c1c115487&gt; in &lt;module&gt;() 1 # 2) I know that when i divide the number 5 i should get 2.5 so lets test the divide function using that. -&gt; 2 assert divide_by_two(5) == 2.5, &quot;The divide_by_two function did not return 2.5 when given 5.&quot; NameError: name &#39;divide_by_two&#39; is not defined . Now start coding to make the tests pass . the idea is to write as little as possible to make the test pass. | . some_data = numpy.empty([39, 2]) . def divide_by_two(vals): return 2.5 . start to refactor to actually work: . some_data = pandas.read_csv(&#39;simple_data.csv&#39;, sep=&#39;,&#39;,header=0) assert some_data.shape == (39,2), &quot;The data hasn&#39;t loaded properly. Size is not 39 x 2&quot; . def divide_by_two(vals): return vals/2.0 assert divide_by_two(5) == 2.5, &quot;The divide_by_two function did not return 2.5 when given 5.&quot; . Now put a piece of my data and my function together . # 3) Now test the two things together. # I know that the third number in my data is 4 and so the divide function should return 2. assert some_data.sample_data[2] == 4, &quot;The data in the third rows of col 2 is no longer 4...&quot; assert divide_by_two(some_data.sample_data[2]) == 2, &quot;The divide_by_two function did not return 2 when given the 3rd element of some_data.sample_data&quot; . Now i can use this function in anger on the whole dataset . divide_by_two(some_data.sample_data) . 0 0.5 1 1.5 2 2.0 3 3.5 4 5.5 5 9.0 6 14.5 7 23.5 8 38.0 9 61.5 10 99.5 11 161.0 12 260.5 13 421.5 14 682.0 15 1103.5 16 1785.5 17 2889.0 18 4674.5 19 7563.5 20 12238.0 21 19801.5 22 32039.5 23 51841.0 24 83880.5 25 135721.5 26 219602.0 27 355323.5 28 574925.5 29 930249.0 30 1505174.5 31 2435423.5 32 3940598.0 33 6376021.5 34 10316619.5 35 16692641.0 36 27009260.5 37 43701901.5 38 70711162.0 Name: sample_data, dtype: float64 .",
            "url": "https://robtheoceanographer.com/blog/markdown/2017/08/28/TDDexample.html",
            "relUrl": "/markdown/2017/08/28/TDDexample.html",
            "date": " • Aug 28, 2017"
        }
        
    
  
    
        ,"post9": {
            "title": "A Guide For Speakers.",
            "content": "All speakers must ask themselves one basic question: Why am I lecturing? What will I be able to get across to the listeners through a lecture that they could not get just by reading a book or working through an online tute? . The answer for me, in part, is that the physical presence of the speaker and the unfolding of the lecture in front of their eyes will make a difference for the audience. Great lecturers not only inform the audience they also engage their imaginations and inspire them learn more. . This is the goal of almost all talks/seminars/lectures/presentations is to give you something that you just can’t get from a book or from a website -&gt; human interaction and inspiration. . I have two basic guidelines/tips that you can use to keep help you keep your audiences interested and to not sound like a pre-recorded audiobook: . 1 – Keep it short and sweet . For most talks you will have tens of minutes and if you are not finished on time the chair will stop you. ALWAYS KEEP TO TIME! . 2 – Tell them a story. . Try it. It works. A story has power. It can connect you to the audience and allow you to put information into their heads. . Your talk should be the trailer, not the whole movie! . So how do you do that, exactly? Well, here’s the template: the ABT… And… But… Therefore… This is the core of any story. e.g. I tried to do this, but it didn’t work, therefore/so I did this cool thing. . Watch this video: . TIPS: Try creating a one sentence statement that provides a few set up details, states the problem, then points to the actions being taken to address it. Some examples: “In my laboratory we study ocean chemistry AND phytoplankton biochemistry, BUT we’ve recently realised the real questions we want to ask require larger spatial and temporal scales, THEREFORE we’ve begun learning matlab to match satellite data to our field data.” or as simple as “We do research BUT realised it’s not the right type THEREFORE we’re taking a different approach.”. . Heaps more can be found here: https://www.facebook.com/ABTshare . More about story telling in science communication can be found here in this science article: http://www.sciencemag.org/content/342/6163/1168.1.full.pdf . . “A great lecture is not a rote mechanical reading of notes, but a kind of dance, in which lecturer and listeners watch, respond to, and draw energy and inspiration from each other.” RICHARD GUNDERMAN .",
            "url": "https://robtheoceanographer.com/blog/markdown/2016/10/03/speaking.html",
            "relUrl": "/markdown/2016/10/03/speaking.html",
            "date": " • Oct 3, 2016"
        }
        
    
  
    
        ,"post10": {
            "title": "Tips For Running a Data Science Group",
            "content": "I’ve built and ran four data science groups/meet ups in the past 10 years or so. Three of the four failed, but each failed in a different way and for different reasons. I now have a list based on this experience and on that one semi-successful group. This list is evolving and is by no means a complete list nor is it suitable for all cases or all people – pick and choose whatever you like from it and ignore the rest. . These are just a few notes of what works for me: . Silence Kills! - Communicate with everyone who bothers to listen. The more people that hear about you and your group the merrier the group will be. For the group to work you have to be in peoples minds and remembered. If people don’t hear from you for a while they will assume you’ve been hit by a bus or the group has failed or both. . | Advertise – Put the group on Twitter and Facebook and Instagram and Slack and everywhere else you can think of! Get your event notifications posted on every email list you can find and spam all of the groups you’re in. “Fight Club” rules do not apply here – you must talk about your group if you want new people attending and if you don’t get new people attending, from time to time, the group will become stale and sour after just a few short months. . | Advertise Really Well – Post notification on many platforms in plain language with lots of information. Don’t rely on someone knowing you personally or finding your tiny wiki page or being on the right email lists at the right time. The person you would most like to be at your group is the person doing the cool stuff that you don’t know about – make it easy for them to find you. Include all the vital information – location, time, duration, accessing the space, where to find more info on the topic… etc. Also, do yourself a favour and Google: “Effective Social Media Tactics”. . | Build Your Own Site - Build yourself a modern and easy to use webpage. Update it all the time. Link it to your social media stuff. Post upcoming events as soon as you know about them. Post agendas for these events – include useful info like what language/tool/package/technique the group will discuss etc etc. Maintain this webpage actively. Do search engine optimisation – people will google you and you want them to find you. . | Build Stability, Clarity, and Certainty – Keep a good archive of past events. People like to look through them for that thing they missed or to find out what your group is like before committing time to attend. Post agendas and time tables of what is coming up. Let people know exactly what they can expect and how much time etc you are asking them for. Post early, post often. Avoid using acronyms and if you do provide a link to Wikipedia or something that explains them. Don’t reference or make jokes about people/orgs or newbs. Show tolerance for all levels and abilities. Host newb events to draw out the shy coders and those who are suffering at the hands of our old friend the imposter syndrome. Accept all points of view and don’t allow the discussions to turn into an argument of “that’s not how I would’ve done it” - get your loudest members to swallow their pride and just say yes every now and again. . | Be Regular - Make your group a regular part of peoples calendars. Make it a routine. Find a cycle and stick to it. Never miss a scheduled meet up date. Never. If someone shows up and the thing isn’t on and there’s no info on the web/email then they are not coming back. Ever. Do this too often and your group will devolve into just you and your mate skiving off for a coffee every month. . | Be Convenient - Organise the time of your group so it’s easy for people to attend - not before 10am and not after 5pm and not on weekends is a good starting rule. Find out what the standing meetings are and avoid them. Make the venue easy to get to and find. Don’t hold it off site. . | Be a Good Community Citizen - look up other groups in the area and invite them over for a joint session. Make sure your events don’t clash with their events or other events in the building. Talk to them. Gain networks of potential speakers and contacts for your members. Acknowledge people around you for their efforts and achievements. . | Get Feedback – Ask for feedback from the people who actually bother to take the time to show up. Act on that feedback. . | Good luck, share your experience, and above all have fun .",
            "url": "https://robtheoceanographer.com/blog/markdown/2016/08/30/StartingAGroup.html",
            "relUrl": "/markdown/2016/08/30/StartingAGroup.html",
            "date": " • Aug 30, 2016"
        }
        
    
  
    
        ,"post11": {
            "title": "In search of a better Tilde.",
            "content": "My good friend Dr. Tom Remenyi just sent me a little LaTeX hack that I thought would be worth sharing. He recently found himself in need of a tilde, that is a ‘~’ and quickly became frustrated with the standard/default LaTeX rendering and as such decided to define his own. Here’s how you can to… . What is a tilde? . tilde (~ , pronounced TILL-duh or TILL-dee and sometimes called a “twiddle” or a “squiggle”) is a foundational mathematical symbol meaning “approximately” and in much of computer science logic means “not.” . Dr.Tom’s instructions: . What follows is a direct quote of his email to me: . For years it has bugged me that using the tilde symbol in latex is a real pain, solutions have been never quite right. I have needed to use it quite a bit in a recent document, so I have found a solution. I thought I would share it, since I like it so much. I hope one of you find this useful. . Below is a line of code you can add to the preamble of any document and it will define a command that will place a sensibly sized and positioned tilde where you want it. (I have quite figured out where I can put it so it always works and doesn’t get overwritten by new packages). Why this isn’t the default (or close to the default), I don’t know, but here it is. . newcommand{ goodtilde}} %create a command that uses a decent ~ (tilde) in-text. . It is used like this throughout the latex of the document: . There are goodtilde50 excellent reasons why this should be the default symbol in latex. . OR . This is the way to put a goodtilde symbol in your text. .",
            "url": "https://robtheoceanographer.com/blog/markdown/2016/06/27/LatexTilde.html",
            "relUrl": "/markdown/2016/06/27/LatexTilde.html",
            "date": " • Jun 27, 2016"
        }
        
    
  
    
        ,"post12": {
            "title": "How to LaTeX - An Absolute Beginners Guide",
            "content": "I’ve been asked a few times recently about how to LaTeX so I thought I’d put a few notes up for everyone to refer back to later. . What is it? . There’s no need to re-invent the wheel here so the best thing is to read the, actually pretty good, Wikipedia pages on Tex and LaTeX. . Having said that I will highlight the purpose of LaTeX (TeX) for those of you, like me, who are too lazy to bother reading those pages - The original TeX was designed with two main goals in mind: to allow anybody to produce high-quality books using a reasonably minimal amount of effort, and to provide a system that would give exactly the same results on all computers, at any point in time. For me, LaTeX is all about the second of those two points - easy reproducibility. . How to install LaTeX? . First of all, there are lots of different types of LaTeX distributions. To simplify your search here are a couple of simple suggests: . There are two parts to a LaTeX installation: a LaTeX distribution (the compiler) and an editor to write the document in (a text editor of your choice): . Editors . You can use any text editor you like but there are a couple that were designed for LaTeX and have built in LaTeX typesetting functionality that is really useful - if you’re just starting out I suggest: TeXShop on a Mac and TeXworks or Texmaker or TeXStudio on Linux, and MiKTeX or Texmaker on Windows. . LaTeX . Linux users: You may already have LaTeX installed on your system. Type which latex at a command prompt to find out - if you have it, it will respond with /usr/bin/latex or something like that. If you don’t you’ll need to install it. Using Fedora run the following as a superuser: yum install texlive . | . If you are using another Linux distribution, you can download LaTeX and install it using your package manager (e.g. apt-get install texlive etc). I’m no linux guru and I was able to install it on my Linux machine without screwing too much up. . Mac users: You can install everything you’ll need from the MacTeX web page http://www.tug.org/mactex/. MacTeX should take care of all of your latex compatibility stuff and should be pretty straight forward. . | Windows users: You can install MiKTeX http://miktex.org/, which is the latex distribution built for Windows. Some helpful notes are found here: http://www.howtotex.com/howto/installing-latex-on-windows/. It should take care of all of your LaTeX compatibility stuff and should be pretty straight forward - your milage may vary, especially with the new Windows OS. . | . The absolute basics . How to create a document with “hello world” as the content: . Go to this link and watch the video http://www.youtube.com/watch?v=kQl2XdBiWNE&amp;feature=related. This video is designed for TexShop but most of this is applicable to TexWorks or MikTeX too. I suggest saving the first document you create into a folder on the desktop called “MyFirstTexDocument”. . Getting started . First of all, if you’re not a coder, don’t get scared by the code and funny symbols and stuff. . Dr. Tom Remenyi put together a very useful training document for LaTeX - mainly designed for marine scientists but it’s forms a useful starting point for all disciplines. Keep this document handy when you’re starting out. It makes a great cheat sheet. I have forked his githib repo and made it available here: https://github.com/RobTheOceanographer/latex_training_document. . The repo needs a little updating and I’ll work on that in the future, but for now there are two core documents of interest to you: . Latex_training_document.pdf (open as normal) | Latex_training_document.tex (open with your latex editor). | . The rest of the files are used by your machine to create the document and you can ignore them for now. Open both files and try to find the bits that are similar between the .tex and the .pdf. A lot of the comments on “how to do blah blah blah” are in the .tex document (comments are the bits that immediately follow a % sign). . I recommend that you start a new document for yourself and use Tom’s Latex training document as a cheat sheet to copy an paste the commands you need to do the things you want. The beauty of it is you can see all the commands in the code, the code that YOU wrote, so it is easier to see why something isn’t working and how to fix it. Also the online help is awesome - Google is your friend. . Installing Packages . Installing new packages is usually a source of frustration for new LaTeX users. On Mac and Linux systems the easiest way to do it is to use your systems package management system (e.g apt-get or yum) to find and install packages. I’ve never really worked with LaTeX on windows you’ll have to do some searching and work this bit out on your own. . To get Tom’s Latex training doc to typeset on Fedora I had to install the following extra packages: . yum install texlive-preprint yum install texlive-threeparttable yum install texlive-multirow . If you can’t find what you’re looking for in your package manager you can search on the Com­pre­hen­sive TeX Archive Net­work (CTAN) website. CTAN is the central place for all LaTeX packages and it currently abut 4952 packages, most of which are free. https://www.ctan.org/ . How to CTAN . If you download a package from CTAN you will need to build the .sty file, put it into a dir that LaTeX can see and then tell LaTeX about it. Below is a rough guide to do this: . search for missing packages here: https://www.ctan.org/search/ . | Run LaTeX on the downloaded &lt;package&gt;.ins file. This should generate a &lt;package&gt;.sty file. If you want to have the documentation for this package then you need to run LaTeX on the &lt;package&gt;.dtx file too. . | Move the &lt;package&gt;.sty and &lt;package&gt;.dvi files to a directory where LaTeX looks for it’s input files. On Linux this is usually somewhere like this: ~/texmf/tex/latex/&lt;package&gt;. . | Update your LaTeX file name database using the texhash command (something like this: texhash ~/texmf) and you’re done. . | . Examples and Inspiration . If you need inspiration check out this stackexchange thread of beautifully typeset documents created in TeX/LaTeX: http://tex.stackexchange.com/questions/1319/showcase-of-beautiful-typography-done-in-tex-friends .",
            "url": "https://robtheoceanographer.com/blog/markdown/2016/01/02/HowToLatex.html",
            "relUrl": "/markdown/2016/01/02/HowToLatex.html",
            "date": " • Jan 2, 2016"
        }
        
    
  
    
        ,"post13": {
            "title": "R Shiny - Getting your data/analysis onto the web with Shiny apps.",
            "content": "A years ago a I worked in a lab that was spending a lot of money using commercial statistical software to analyse Phytoplankton physiological data collected with a PAM and a group of PhD students, including me, decided that we could do the same analysis using the free and open statistical programming language R. We produced a set of functions to do all of the analysis needed by the team but after a while we realised that people were not using them as they weren’t comfortable with the command line or with R. Therefore we set about building a simple Graphical User Interface designed specifically to use our R analysis tools using a Shiny app. Here are a few notes on how to get started in Shiny and a link to the PAM Light Curve analyser we built. . What is it? . The Shiny CRAN page has this to say: “shiny: Web Application Framework for R makes it incredibly easy to build interactive web applications with R. Automatic “reactive” binding between inputs and outputs and extensive pre-built widgets make it possible to build beautiful, responsive, and powerful applications with minimal effort.” . But to me Shiny’s greatest value is that there’s no need to re-invent the wheel in html or JavaScript as a lot of the components that you want for a simple app have already been done before and can be copied directly from others. This is super easy in Shiny as they have a great Gallery of examples - http://shiny.rstudio.com/gallery/ . For some inspiration see the User Showcase: https://www.rstudio.com/products/shiny/shiny-user-showcase/ . Here is an embedded example from the Shiny Gallery: . An example from the gallery. How to Shiny? . First of all, there are lots of different ways to start using Shiny but the best way is to take half an hour and work through the on-line tutorial here: http://shiny.rstudio.com/tutorial/. . Structure of a Shiny app . Basically, there are two components to a Shiny app: 1) a user-interface script (ui.R), and 2) a server script (server.R). The ui.R script controls the layout and appearance of your app. The server.R script contains the instructions that your computer needs to build your app, including loading any data and or doing any analysis or plotting etc that you want to do.See the tute for an example of each . Running a shiny app . As all Shiny apps have the same basic structure: two core R scripts saved together in a directory. The easiest way to run your app is to make a new directory and saving a ui.R and server.R file inside it. Each app will need its own unique directory. . You run a Shiny app by giving the name of its directory to the function runApp in R. For example if your Shiny app is in a directory called my_app, run it with the following code inside an R shell: . &gt; library(shiny) . &gt; runApp(&quot;my_app&quot;) . Sharing the Shiny - Hosting and Deployment . Up to this point everything you’ve been doing is probably isolated on your own computer and not shareable or on the internet. You can, as I often do, just use these shiny apps as simple GUI interfaces to your analysis and share them with your friends on Git Hub or if you want to you can put your Shiny app on the web for your friends to use. To do this you can choose to deploy on your own R Shiny servers or use the RStudio Shiny hosting service for a small fee. . The PAM Processor App . This is a project that allows users to import their PAM fluorometer data into the R programming language and calculate photosynthetic parameters as per Platt et al 1980 and Ralph et al. . This code is brought to you by Rob Johnson, Simon Reeves, Shihong Lee, and Emma Flukes. We are (or were) PhD students at the University of Tasmania and we were fed up with the ridiculous amount of time and money it was taking to analyse PAM Rapid Light Curves using clunky and expensive stats software (copy… paste… copy… paste… SPSS… boooo!). So, we built the PAM Processor! using the free and open stats scripting language R and the web application tool Shiny. . At the moment this is release v1.0 of the PAM Processor! as of June 2015 and it’s available for download here: https://github.com/RobTheOceanographer/pam_in_R/releases .",
            "url": "https://robtheoceanographer.com/blog/markdown/2015/09/03/shiny.html",
            "relUrl": "/markdown/2015/09/03/shiny.html",
            "date": " • Sep 3, 2015"
        }
        
    
  
    
        ,"post14": {
            "title": "Art meets Science",
            "content": "ex Oceano - we are from the ocean - the ocean sustains us from Nick Roden on Vimeo. . Two years in the making and the ex Oceano symphony is out! . ex Oceano is a creative venture in sound – a collaborative expression between disciplines and understandings. . The wonderful Nick Roden and I expressed our understanding of science to a composer - Matthew Dewey then the composer expressed that science back to us in sound – by making a symphony. . This made for a way to generate a different experience of ocean science and promote understanding of the role of the ocean in supporting life. . For more info on ex Oceano visit http://www.lynchpin.org.au/our-projects/ex-oceano/ .",
            "url": "https://robtheoceanographer.com/blog/markdown/2015/06/04/Lynchpin.html",
            "relUrl": "/markdown/2015/06/04/Lynchpin.html",
            "date": " • Jun 4, 2015"
        }
        
    
  
    
        ,"post15": {
            "title": "How to calibrate a Turner Fluorometer for phytoplankton Chlorophyll a",
            "content": "Chlorophyll a (chl) measurements are a useful estimate of algal biomass and have been used extensively for decades. Turner designs (http://www.turnerdesigns.com/ fluorometers are used extensively to quantify chl as they are fast, reliable, and relatively cheap. Nevertheless, errors can be introduced if the fluorometer isn’t calibrated with a commercially available chlorophyll a standard (e.g. crystallised Chlorophyll a from Anacystis nidulans algae from Sigma Chemical Company) regularly. The procedure described here is appropriate for algal chl in the marine environment. Scientists who employ this or other methods to measure pigments should make themselves aware of the issues that surround fluorescence based techniques and make appropriate decisions about the use of this technology for their application based on the scientific requirements and constraints of their individual programs. . How often? . If used in the laboratory your fluorometer should be calibrated every 6 months. If the fluorometer is to be taken into the field then it should be calibrated before and after each voyage/expedition. . Turner calibration . The method for calibrating a Turner fluorometer is quite straight forward and is well documented by Turner themselves. – see this document for their 10-AU model. The part that is not often documented is how do you independently determine the concentration of your chlorophyll standard in order to do this calibration in the first place. That is what I hope to show you how to do here. . Spectophotometric estimation of chlorophyll concentrations . This is a brief method for estimating chlorophyll concentrations based on a samples spectral absorption, following Jeffrey and Humphrey 1975: . Make up solutions of varying chlorophyll concentrations in 100% acetone (see Jeffrey and Humphrey 1975 for info on other solvents). I usually make three (a low, middle, and high concentration) for doing a calibration run. | Using a UV/Vis spectrophotometer measure the absorption of your chlorophyll standard at 664nm, 647nm, 630nm, 750nm. I’ll leave it up to you to work out how to make these measurements as all spectro’s have different software etc etc… | Calculate the extinction coefficients at 664nm, at 647nm, and at 630nm using the following eq: | . . where x denotes the wavelength of interest (e.g. 664 or 647 or 630nm), A is the absorption value, and E is the resulting extinction coefficient. . Using these extinction coefficients calculate chlorophyll a concentration using equation 4 from Jeffrey and Humphrey 1975 as follows: . . where the resulting units are based on your abs measurement units but are likely to be ug/mL and so need to be converted to the more common ug/L. There are also equations for chl b and c should you be interested in them. . Now you know the chl concentration for each of your solutions you can now use this information to calibrate your Turner fluorometer. . Bibliography: . Jeffrey, SW T., and G. F. Humphrey. “New spectrophotometric equations for determining chlorophylls a, b, c1 and c2 in higher plants, algae and natural phytoplankton.” Biochem Physiol Pflanz BPP (1975). PDF .",
            "url": "https://robtheoceanographer.com/blog/markdown/2014/12/01/TurnerCalibration.html",
            "relUrl": "/markdown/2014/12/01/TurnerCalibration.html",
            "date": " • Dec 1, 2014"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Rob Johnson is a jack-of-all trades, Doctor of Philosophy, data scientist, recovering oceanographer, doer and oddball. Described by a colleague as “…good at turning data into insights. Rob communicates complex matters effectively and is comfortable saying ‘I don’t know, but I know how to find out’”, he also happens to be genuinely excited by scientific challenges and his passion motivates others to do their best. With a string of successful science projects under his belt, Rob has become known for his warm and engaging personality, authentic insights, and wealth of knowledge and experience that he applies to new challenges. Though all of this he has developed a great “BS” detector, which allows him to make progress where others get stuck. . . Rob currently works on aviation related data science problems: for example building state of the art nowcasts for lightning and thunderstorms, automating business process and optimising decision making for Terminal Aerodrome Forecasts (TAFS), and machine learning based weather nowcasting. Rob has worked on broad range of projects from measuring Antarctic Phytoplankton photo-physiology, algorithm development for space based remote sensing, weather modelling using machine learning and AI, to High Performance Computing system administration and data engineering for earth system modelling. . Being interested in all fields of science, and instantly memorable, speaks volumes about how Rob goes about life and represents himself to the world, believing wholeheartedly that science can be made useful through state of the art science being combined with advanced software engineering. . Rob lives in Hobart, Tasmania, Australia. .",
          "url": "https://robtheoceanographer.com/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Papers",
          "content": "Here is a list of my scientific publications for you to enjoy: . Nicole Hellessey, R. Johnson, J.A. Ericson, P.D. Nichols, S. Kawaguchi , S. Nicol, N. Hoem, and P. Virtue, “Antarctic Krill Lipid and Fatty acid Content Variability is associated to Satellite Derived Chlorophyll a and Sea Surface Temperatures”, 2019, Nature: Scientific Reports, 2020, doi:10.1038/s41598-020-62800-7. GetThePDF . | Nicholas Pittman, P.G. Strutton, R. Johnson, R.J. Matear, “An assessment and improvement of satellite ocean color algorithms for the tropical Pacific Ocean” Journal of Geophysical Research: Oceans, 2019, doi:10.1029/2019JC015498. GetThePDF . | David Antoine, T. Schroeder, M. Slivkoff, W. Klonowski, M. Doblin, J. Lovell, D. Boadle, B. Baker, E. Botha, C. Robinson, E. King, P. Fearns, N. Hardman-Mountford, R. Johnson, N. Cherukuru, A. Dekker, T. Malthus, R. Mitchell, P. Thompson, P. Van Ruth, “FINAL REPORT: The IMOS “Radiometry Task Team””, Integrated Marine Observing System, 2018. GetThePDF . | Allen Pope, P. Wagner, R. Johnson, J.D. Shutler, J. Baeseman, and L. Newman, “Community review of Southern Ocean satellite data needs”, Antarctic Science, 2016, DOI: 10.1017/S0954102016000390. GetThePDF . | Andrew Davidson, J. McKinlay, K. Westwood, P.G. Thomson, R. van den Enden, M. de Salas, R. Johnson, K. Berry, B. Tilbrook., “Enhanced CO2 concentrations change the structure and function of Antarctic marine microbial communities” Marine Ecology Progress Series, 2016. DOI: 10.3354/meps11742 GetThePDF . | Jake R. Wallis, K.M. Swadling, J.D. Everett, I.M. Suthers, H.J. Jones, P.J. Buchanan, C.M. Crawford, L.C. James, R. Johnson, K.M. Meiners, and P. Virtue., (2015) “Zooplankton abundance and biomass size spectra in the East Antarctic sea-ice zone during the winter–spring transition.” Deep Sea Research Part II: Topical Studies in Oceanography. DOI:10.1016/j.dsr2.2015.10.002 GetThePDF . | Ruhi S. Humphries, R. Schofield, M.D. Keywood, J. Ward, J.R. Pierce, C.M. Gionfriddo, M.T. Tate, D.P. Krabbenhoft, I.E. Galbally, S.B. Molloy, A.R. Klekociuk, P.V. Johnston, K. Kreher, A.J. Thomas, A.D. Robinson, N.R.P. Harris, R. Johnson, and S.R. Wilson., (2015) “Boundary layer new particle formation over East Antarctic sea ice - possible Hg catalysed nucleation?”, Atmospheric Chemistry and Physics, DOI:10.5194/acp-15-13339-2015 GetThePDF . | Robert Johnson, P.G Strutton, S.W Wright, A McMinn, and K Meiners., (2013), “Three Improved Satellite Chlorophyll Algorithms for the Southern Ocean”, Journal of Geophysical Research – Oceans, DOI:10.1002/jgrc.20270 GetThePDF . | Katherina Petrou, R. Hill, M.A. Doblin, A. McMinn, R. Johnson, S.W. Wright, and P.J. Ralph., (2011) “Photoprotection of sea-ice microalgal communities from the east Antarctic”, Journal of Phycology, DOI:10.1111/j.1529-8817.2010.00944.x GetThePDF . | .",
          "url": "https://robtheoceanographer.com/blog/papers/",
          "relUrl": "/papers/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://robtheoceanographer.com/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}